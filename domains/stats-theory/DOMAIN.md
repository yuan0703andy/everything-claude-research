# Statistical Theory Domain Knowledge

**Last Updated**: 2026-01-27
**Maintained By**: Research Team

---

## Domain Overview

Statistical theory studies the mathematical properties and fundamental limits of inference methods. The core question is: Given a finite sample, how accurately can we infer unknown parameters or test hypotheses? The key concept is **minimax optimality** - best performance under worst-case scenarios.

---

## Core Theoretical Frameworks

### 1. Decision Theory (Wald, Le Cam)
**Core idea**: Statistical inference as a decision problem
- **Parameter space** Î˜
- **Decision space** D
- **Loss function** L(Î¸, d)
- **Risk** R(Î¸, Î´) = E[L(Î¸, Î´(X))]
- **Minimax risk**: inf_Î´ sup_Î¸ R(Î¸, Î´)

**Key references**:
- Wald (1950) - Statistical Decision Functions
- Le Cam (1986) - Asymptotic Methods in Statistical Decision Theory

### 2. Minimax Theory (Donoho, Johnstone)
**Core idea**: Optimality in the worst case
- **Minimax rate**: n^(-Î±) where Î± depends on problem smoothness
- **Lower bound**: Prove that no estimator can be faster than this
- **Upper bound**: Construct an estimator that achieves this rate

**Typical results**:
- Nonparametric regression: n^(-2Î²/(2Î²+d)) where Î²=smoothness, d=dimension
- High-dimensional sparse estimation: (s log p)/n where s=sparsity, p=dimension

**Key references**:
- Donoho & Johnstone (1994) - Ideal spatial adaptation via wavelet shrinkage
- Donoho & Liu (1991) - Geometrizing rates of convergence, I-III

### 3. High-Dimensional Statistics (Wainwright, BÃ¼hlmann)
**Core idea**: Statistical inference when p >> n
- **Sparsity**: Only s << p variables are important
- **Restricted eigenvalue condition**: Guarantees uniqueness
- **Incoherence**: Low correlation in design matrix

**Typical methods**:
- Lasso: â„“1 penalty
- Dantzig selector: â„“âˆž constraint
- SCAD, MCP: Non-convex penalties

**Key references**:
- Wainwright (2019) - High-Dimensional Statistics: A Non-Asymptotic Viewpoint
- BÃ¼hlmann & van de Geer (2011) - Statistics for High-Dimensional Data

### 4. Empirical Process Theory (van der Vaart, Wellner)
**Core idea**: Uniform laws for stochastic processes
- **Glivenko-Cantelli**: Uniform LLN
- **Donsker**: Functional CLT
- **Bracketing & covering numbers**: Complexity measures

**Applications**:
- M-estimation
- Maximum likelihood
- Nonparametric methods

**Key references**:
- van der Vaart & Wellner (1996) - Weak Convergence and Empirical Processes

### 5. Information Theory (Fano, Assouad)
**Core idea**: Information-theoretic lower bounds
- **Fano's inequality**: H(Î¸|X) â‰¥ log|Î˜| - I(Î¸;X) - 1
- **Assouad's lemma**: Hypercube construction
- **Le Cam's method**: Two-point testing

**Applications**: Proving minimax lower bounds

**Key references**:
- Yu (1997) - Assouad, Fano, and Le Cam
- Tsybakov (2009) - Introduction to Nonparametric Estimation

---

## Proof Techniques Toolbox

### Concentration Inequalities

**Hoeffding's Inequality**:
```
P(|XÌ„ - Î¼| â‰¥ t) â‰¤ 2 exp(-2ntÂ²/(b-a)Â²)
```
Use case: Bounded random variables

**Bernstein's Inequality**:
```
P(|XÌ„ - Î¼| â‰¥ t) â‰¤ 2 exp(-ntÂ²/(2ÏƒÂ² + 2bt/3))
```
Use case: Sub-exponential tails

**McDiarmid's Inequality**:
```
P(|f(Xâ‚,...,Xâ‚™) - E[f]| â‰¥ t) â‰¤ 2 exp(-2tÂ²/Î£cáµ¢Â²)
```
Use case: Functions with bounded differences

**Usage scenarios**:
- Proving estimator concentration
- Oracle inequalities
- Deviation bounds

### Empirical Process Techniques

**Symmetrization**:
```
E[sup_f |Pâ‚™f - Pf|] â‰¤ 2E[sup_f |Pâ‚™f - P'â‚™f|]
```

**Chaining**:
```
E[sup_{fâˆˆF} |Pâ‚™f|] â‰¤ Câˆ«â‚€^âˆž âˆšlog N(Îµ, F, Lâ‚‚(Pâ‚™)) dÎµ
```

**Usage scenarios**:
- Uniform convergence
- M-estimation consistency
- Donsker theorems

### Information-Theoretic Lower Bounds

**Fano's Method**:
```
1. Construct parameter set {Î¸â‚,...,Î¸â‚˜} with large m
2. Compute KL divergence: KL(PÎ¸áµ¢ || PÎ¸â±¼) â‰¤ I
3. Apply Fano: P(error) â‰¥ 1 - (I + log 2)/log m
```

**Assouad's Lemma**:
```
1. Construct hypercube {Î¸áµ¥ : v âˆˆ {0,1}áµˆ}
2. Ensure ||Î¸áµ¤ - Î¸áµ¥|| â‰¥ Î” when Hamming distance = 1
3. Lower bound: Râ‚™ â‰¥ cÎ”Â²d Ã— avg error in single coordinate testing
```

**Le Cam's Two-Point Method**:
```
1. Construct two hypotheses Hâ‚€: Î¸ = Î¸â‚€ vs Hâ‚: Î¸ = Î¸â‚
2. Compute total variation: TV(Pâ‚€, Pâ‚) = 1 - exp(-KL/2)
3. Lower bound: minimax risk â‰¥ (||Î¸â‚€-Î¸â‚||Â²/4) Ã— (1 - TV)
```

**Usage scenarios**:
- Proving minimax lower bounds
- Establishing fundamental limits
- Proving optimality of methods

### Change of Measure

**Girsanov-type arguments**:
```
dQ/dP = exp(stuff)
Use Q for favorable analysis, then change back
```

**Usage scenarios**:
- Sequential analysis
- Change point detection
- Testing with nuisance parameters

---

## Methodological Standards

### Three Levels of Theoretical Contribution

#### Gold Standard (Top-tier contribution)
âœ… **Must include**:
1. **New phenomenon identification**: Discover new statistical phenomena
2. **Minimax rate derivation**: Derive the minimax rate
3. **Lower bound proof**: Prove lower bound (Fano/Assouad/Le Cam)
4. **Optimal procedure**: Construct a method that achieves the rate
5. **Phase transition characterization**: Characterize fundamental limits

**Example**: Donoho & Johnstone (1994) on wavelet thresholding
- Phenomenon: Near-ideal spatial adaptation
- Rate: n^(-2Î²/(2Î²+1))
- Lower bound: Proved via oracle inequality
- Optimal: VisuShrink, SureShrink
- Phase transition: Risk ellipse geometry

#### Good (Excellent contribution)
âœ… **Must include**:
1. **New method**: Propose a new method
2. **Rate analysis**: Analyze convergence rate
3. **Comparison**: Compare with existing methods
4. **Simulations**: Numerical evidence
5. **Assumptions justified**: Clearly explain assumptions

#### Acceptable
âœ… **Must include**:
1. **Extension**: Extend known results to new settings
2. **Rigorous proof**: Rigorous proofs
3. **Non-trivial**: Non-obvious extension

---

## Publication Standards

### Top Journals (Tier 1)

**Annals of Statistics**
- Highest standard, requires major theoretical contribution
- Expectation: Minimax optimality + lower bound
- Review cycle: 6-12 months
- Typical article length: 30-50 pages

**Journal of the Royal Statistical Society Series B (JRSSB)**
- Emphasizes methodology with theory
- Expectation: New method + asymptotic properties
- Review cycle: 4-8 months

**Biometrika**
- Classical statistical theory
- Expectation: Rigorous asymptotic analysis

### Second Tier

**Electronic Journal of Statistics (EJS)**
- Open access, faster review
- Still requires rigorous proofs

**Bernoulli**
- Probability + Statistics interface

**Journal of Machine Learning Research (JMLR)**
- Accepts statistical learning theory

---

## Review Standards and Common Comments

### Major Concerns (Leading to rejection)

ðŸš© **No lower bound**
```
"The authors claim their method is optimal, but provide no lower bound.
Without a matching lower bound, we cannot assess optimality."
```
**How to avoid**: Always include a lower bound, or at least compare with known lower bounds

ðŸš© **Unrealistic assumptions**
```
"The assumption of sub-Gaussian errors with known variance is too strong.
Real data rarely satisfies this."
```
**How to avoid**:
- Clearly explain the necessity of assumptions
- Provide robustness analysis
- Consider weaker assumptions

ðŸš© **Proof has gaps**
```
"In the proof of Theorem 2, the authors claim that term Tâ‚ƒ = o(nâ»Â¹/Â²)
without justification. This is not obvious and requires proof."
```
**How to avoid**:
- Prove all non-trivial steps
- Use "By [standard result], we have..." with citations
- Provide detailed proofs in appendix

ðŸš© **Contribution unclear**
```
"This result follows easily from [existing paper] by standard techniques.
The contribution is incremental."
```
**How to avoid**:
- Clearly state novelty
- Explicitly compare with existing results
- Emphasize technical challenges

ðŸš© **Simulation contradicts theory**
```
"Figure 1 shows the estimator performs worse than predicted by Theorem 1.
This suggests the constants in the rate may be wrong."
```
**How to avoid**:
- Simulations should be consistent with theory
- Explain discrepancies (constants, finite sample effects)
- Don't cherry-pick results

### Minor Concerns (Requiring revision)

âš ï¸ **Notation inconsistent**
```
"The authors use Î¸ for both the parameter and an angle in Section 3."
```

âš ï¸ **Some edge cases not discussed**
```
"What happens when p = n? The theorem assumes p < n but many
applications have p â‰¥ n."
```

âš ï¸ **Comparison incomplete**
```
"The authors compare with [Method A] but not [Method B] which is
more relevant."
```

âš ï¸ **Writing unclear**
```
"The sentence 'It follows that...' on line 234 is ambiguous."
```

### Positive Signals (Leading to acceptance)

âœ… **Novel phenomenon**
```
"The identification of the phase transition boundary is new and insightful."
```

âœ… **Tight bounds**
```
"The matching upper and lower bounds establish the exact minimax rate."
```

âœ… **Innovative technique**
```
"The proof technique using empirical process theory is novel and
may find applications elsewhere."
```

âœ… **Well-positioned**
```
"The paper clearly positions itself in the literature and addresses
a genuine gap."
```

---

## Notation and Terminology Conventions

### Standard Notation

**Samples and parameters**:
- `n`: Sample size
- `p`: Dimension
- `d`: Dimension (sometimes used for spatial dimension)
- `Î¸`: Parameter (typically a vector)
- `Î¸â‚€`: True parameter
- `Î¸Ì‚`: Estimator

**Distributions and expectations**:
- `P`, `Q`: Probability measures
- `P_Î¸`: Parameterized distribution
- `E_Î¸`: Expectation under P_Î¸
- `X ~ P`: X follows distribution P

**Risk and loss**:
- `L(Î¸, d)`: Loss function
- `R(Î¸, Î´)`: Risk of estimator Î´
- `â„“_p` norms: `||Â·||_p`

**Sequence notation**:
- `a_n = O(b_n)`: a_n â‰¤ Cb_n for some constant C
- `a_n = o(b_n)`: a_n/b_n â†’ 0
- `a_n â‰ b_n`: a_n = Î˜(b_n), i.e., Câ‚b_n â‰¤ a_n â‰¤ Câ‚‚b_n
- `a_n â‰² b_n`: a_n â‰¤ Cb_n (weak inequality)

**Probability notation**:
- `P(event)`: Probability
- `Pâ‚™`: Empirical measure Pâ‚™f = (1/n)Î£f(Xáµ¢)
- `Gâ‚™`: Empirical process Gâ‚™f = âˆšn(Pâ‚™ - P)f

### Domain-Specific Terms

**Chinese â†’ English Mapping**:
- ä¼°è¨ˆé‡ â†’ Estimator
- æ”¶æ–‚é€Ÿåº¦ â†’ Rate of convergence
- æ¼¸è¿‘æ•ˆçŽ‡ â†’ Asymptotic efficiency
- æ¥µå°æ¥µå¤§æœ€å„ª â†’ Minimax optimal
- è‡ªé©æ‡‰ â†’ Adaptive
- ç¨€ç–æ€§ â†’ Sparsity
- æ­£å‰‡åŒ– â†’ Regularization

---

## Typical Research Problem Patterns

### Pattern 1: Estimation Problem

**Structure**:
1. **Setup**: Observe X ~ P_Î¸, want to estimate Î¸
2. **Loss**: L(Î¸, Î¸Ì‚) = ||Î¸ - Î¸Ì‚||Â²
3. **Goal**: Characterize minimax risk inf_Î¸Ì‚ sup_Î¸ E[L(Î¸, Î¸Ì‚)]

**Key Questions**:
- What is the minimax rate?
- Can we achieve it adaptively?
- What is the price of adaptation?

**Example**: Nonparametric regression
- Observe: Yáµ¢ = f(xáµ¢) + Îµáµ¢
- Estimate: f âˆˆ Î£(Î², L) (HÃ¶lder class)
- Rate: n^(-2Î²/(2Î²+d))

### Pattern 2: Testing Problem

**Structure**:
1. **Hypotheses**: Hâ‚€: Î¸ âˆˆ Î˜â‚€ vs Hâ‚: Î¸ âˆˆ Î˜â‚
2. **Separation**: inf{||Î¸â‚€ - Î¸â‚|| : Î¸â‚€ âˆˆ Î˜â‚€, Î¸â‚ âˆˆ Î˜â‚} = Î”â‚™
3. **Goal**: Characterize detection boundary Î”â‚™

**Key Questions**:
- At what separation Î”â‚™ can we distinguish Hâ‚€ from Hâ‚?
- Is there a phase transition?
- What is the optimal test statistic?

**Example**: Sparse signal detection
- Hâ‚€: Î¸ = 0 vs Hâ‚: ||Î¸||â‚€ â‰¤ s, ||Î¸||â‚‚Â² â‰¥ Î”â‚™
- Detection boundary: Î”â‚™ = âˆš(s log(p/s)/n)

### Pattern 3: Adaptation Problem

**Structure**:
1. **Unknown smoothness**: f âˆˆ Î£(Î², L) but Î² unknown
2. **Goal**: Construct estimator that adapts to unknown Î²
3. **Price**: Compare adaptive rate to oracle rate

**Key Questions**:
- Can we achieve oracle rate without knowing Î²?
- What is the price of adaptation (log factor)?
- Is full adaptation possible?

---

## Research Proposal Evaluation Standards

### Theorist's Checklist

When proposing a new hypothesis, self-check:

- [ ] **Problem well-defined**: Parameter space, model, objectives clear
- [ ] **Novelty clear**: What distinguishes it from existing literature
- [ ] **Minimax formulation**: Can it be formulated as a minimax problem
- [ ] **Rate prediction**: What is the expected minimax rate
- [ ] **Lower bound strategy**: How to prove the lower bound (Fano? Assouad? Le Cam?)
- [ ] **Upper bound strategy**: How to construct an estimator that achieves the rate
- [ ] **Assumptions reasonable**: Are the assumptions too strong
- [ ] **Edge cases identified**: Boundary cases (p=n? s=0? Î²=âˆž?)

### Experimentalist's Checklist

Assessing feasibility:

- [ ] **Computational complexity**: O(?) - Is it practically feasible
- [ ] **Algorithm well-defined**: Is there a clear algorithm
- [ ] **Simulation plan**: How to verify the theoretical rate
- [ ] **Comparison baselines**: Which existing methods to compare with
- [ ] **Data availability**: What data is needed for verification
- [ ] **Implementation difficulty**: Assessment of implementation difficulty

### Methodologist's Checklist

Methodological review:

- [ ] **Proof technique identified**: What techniques are used for proof
- [ ] **Technical conditions verified**: Check regularity conditions
- [ ] **Constants tracked**: Track not just rates, but also constants
- [ ] **Uniform vs pointwise**: Are results uniform or pointwise
- [ ] **Adaptation considered**: Is the adaptive setting considered
- [ ] **Reproducibility**: Can the proof be reproduced by others

---

## Common Pitfalls (Red Flags)

### ðŸš© Theoretical Pitfalls

**"Hand-waving" proof**:
```
âŒ "By standard concentration inequalities, we have..."
   (Doesn't specify which inequality, whether conditions are satisfied)

âœ… "By Hoeffding's inequality (since Îµáµ¢ are bounded in [-1,1]),
   P(|XÌ„ - Î¼| â‰¥ t) â‰¤ 2exp(-2ntÂ²)"
```

**Missing constants**:
```
âŒ "The rate is O(nâ»Â¹/Â²)"
   (Constants may be large, leading to poor practical performance)

âœ… "||Î¸Ì‚ - Î¸||Â² â‰¤ (ÏƒÂ²p/n)(1 + o(1))"
   (Provides leading constant)
```

**Claiming optimality without proof**:
```
âŒ "Our method is minimax optimal."
   (No lower bound)

âœ… "Our method achieves the minimax rate nâ»Â²áµ/(Â²áµâºáµˆ),
   matching the lower bound of [Ref]."
```

### ðŸš© Methodological Pitfalls

**Over-claiming**:
```
âŒ "Our method works for all distributions."
   (Usually requires some regularity)

âœ… "Our method works for distributions satisfying [specific condition]."
```

**Ignoring computational cost**:
```
âŒ Proposing NP-hard optimization without discussing computational aspect

âœ… "While the optimization is NP-hard in worst case, we show that
   under [condition], a polynomial-time algorithm achieves the rate."
```

### ðŸš© Experimental Pitfalls

**Unrealistic simulations**:
```
âŒ Only test on Gaussian noise when theory assumes sub-Gaussian

âœ… Test on: Gaussian, t-distribution, contaminated Gaussian
```

**Cherry-picked results**:
```
âŒ Only show cases where your method wins

âœ… Show failure modes and explain why
```

---

## Recommended Learning Path

### Beginner Must-Reads (Fundamentals)
1. **Van der Vaart (1998)** - Asymptotic Statistics
2. **Lehmann & Casella (1998)** - Theory of Point Estimation
3. **Wasserman (2006)** - All of Nonparametric Statistics

### Advanced (Minimax Theory)
1. **Tsybakov (2009)** - Introduction to Nonparametric Estimation
2. **GinÃ© & Nickl (2016)** - Mathematical Foundations of Infinite-Dimensional Statistical Models

### High-Dimensional Statistics
1. **Wainwright (2019)** - High-Dimensional Statistics: A Non-Asymptotic Viewpoint
2. **BÃ¼hlmann & van de Geer (2011)** - Statistics for High-Dimensional Data

### Empirical Processes
1. **Van der Vaart & Wellner (1996)** - Weak Convergence and Empirical Processes

---

## Summary: Core Principles

Golden rules for statistical theory research:

1. **Minimax is the gold standard** - Always think about worst-case
2. **Lower bounds matter** - Don't claim optimality without proof
3. **Constants matter** - Not just rates
4. **Assumptions must be justified** - And tested
5. **Rigorous proofs required** - No hand-waving
6. **Comparison with literature essential** - Position your work
7. **Reproducibility paramount** - Others should be able to verify

**Keys to success in this field**:
- Solid mathematical foundation (measure theory, functional analysis)
- Innovative proof techniques
- Deep understanding of classical results
- Connection to applied problems

---

**This domain knowledge should be injected into all agents working on statistical theory projects.**
